{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from time import sleep, time\n",
    "\n",
    "# Data sheets \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Language\n",
    "from googletrans import Translator\n",
    "# Language detection\n",
    "import langdetect\n",
    "langdetect.DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \".\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "\n",
    "# Where we will store data from all sources after combining it\n",
    "ALL_DATA_DIR = os.path.join(DATA_DIR, \"combined\")\n",
    "# This will hold the converted versions of the PDF documents\n",
    "SPLIT_TEXT_FILES_DIR = os.path.join(ALL_DATA_DIR, \"text_files_split\")\n",
    "# This will hold everything related to translating text, including the results of the translations\n",
    "TRANSLATIONS_DIR = os.path.join(ALL_DATA_DIR, \"translations\")\n",
    "\n",
    "if not os.path.exists(TRANSLATIONS_DIR):\n",
    "    os.makedirs(TRANSLATIONS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_delimiter = \"\\n\\n\" + \"=\" * 20 + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Type</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accountability for Perpetrators: UN Officials ...</td>\n",
       "      <td>Official Statement</td>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>UN Special Representative of the Secretary-Gen...</td>\n",
       "      <td>https://www.globalr2p.org/wp-content/uploads/2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title                Type  \\\n",
       "id                                                                          \n",
       "0   Accountability for Perpetrators: UN Officials ...  Official Statement   \n",
       "\n",
       "         Date                                             Source  \\\n",
       "id                                                                 \n",
       "0  2019-11-07  UN Special Representative of the Secretary-Gen...   \n",
       "\n",
       "                                                 link  \n",
       "id                                                     \n",
       "0   https://www.globalr2p.org/wp-content/uploads/2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speeches = pd.read_csv(os.path.join(ALL_DATA_DIR, \"document_data.csv\"), index_col=\"id\")\n",
    "df_speeches.Date = pd.to_datetime(df_speeches[\"Date\"])\n",
    "df_speeches.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate /detect languages in paragraphs\n",
    "\n",
    "Not all texts are in English (or not all of them. We can try and automatically detect which ones contain text in another language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing_translations = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 470 ms, sys: 26.7 ms, total: 497 ms\n",
      "Wall time: 496 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Translation takes a while, so if we have don it before, load\n",
    "if load_existing_translations:\n",
    "    with open(os.path.join(TRANSLATIONS_DIR, \"lang_translations_df.pkl\"), \"rb\") as fp:\n",
    "        df_translations = pickle.load(fp)\n",
    "        translations = df_translations.to_dict(\"records\")\n",
    "\n",
    "else:\n",
    "    # Instantiate googletrans translator object\n",
    "    translator = Translator(timeout=5)\n",
    "    from json import JSONDecodeError\n",
    "    from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "\n",
    "    translations = []\n",
    "    nfiles = len(os.listdir(SPLIT_TEXT_FILES_DIR))\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(SPLIT_TEXT_FILES_DIR)):\n",
    "        with open(os.path.join(SPLIT_TEXT_FILES_DIR, filename), \"r\") as fp:\n",
    "            text = fp.read()\n",
    "\n",
    "        paragraphs = list(text.split(paragraph_delimiter))\n",
    "\n",
    "        for j, paragraph in enumerate(paragraphs):\n",
    "\n",
    "            gt_trans = None\n",
    "            ld_lang = None\n",
    "\n",
    "            # googletrans: need retry logic since it fails sometimes\n",
    "            n_tries = 3\n",
    "            for i_try in range(n_tries):\n",
    "                try:\n",
    "                    gt_trans = translator.translate(paragraph)\n",
    "                    break\n",
    "                except JSONDecodeError:\n",
    "                    if i_try < n_tries - 1:\n",
    "                        err_text = \"Will re-connect and retry\"\n",
    "                        print(f\"\\tFailed to translate for {filename}, paragraph {j+1}. {err_text}\")\n",
    "\n",
    "                        # Google has blocked the current IP address, switch that\n",
    "                        subprocess.run([\"nordvpn\", \"d\"], check=True)\n",
    "                        subprocess.run([\"nordvpn\", \"c\"], check=True)\n",
    "                        print(f\"\\tConnected successfully, attempt {i_try+1} | {n_tries}\")\n",
    "                        # Sometimes it takes a while for a connection to be actually established, hang for a bit\n",
    "                        sleep(1)\n",
    "\n",
    "                        # Important: need to re-instantiate the translator object after changing IP\n",
    "                        translator = Translator(timeout=5)\n",
    "                    else:\n",
    "                        print(f\"\\tgoogletrans could not detect languages in {filename}, paragraph {j+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\\tSomething went wrong {e}. Will retry.\")\n",
    "\n",
    "            # Langdetect\n",
    "            try:\n",
    "                ld_lang = langdetect.detect_langs(paragraph)\n",
    "            except LangDetectException:\n",
    "                print(f\"\\tlangdetect could not detect languages in {filename}, paragraph {j+1}\")\n",
    "\n",
    "            row = {\n",
    "                \"id\": int(filename.split(\".\")[0]),\n",
    "                \"paragraph\": j,\n",
    "                \"gt_trans\": gt_trans,\n",
    "                \"ld_lang\": ld_lang,\n",
    "            }\n",
    "\n",
    "            translations.append(row)\n",
    "\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print(f\"Done with {i+1} out of {nfiles} in {time() - start:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translations = pd.DataFrame(translations)\n",
    "\n",
    "with open(os.path.join(TRANSLATIONS_DIR, \"lang_translations_df.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(df_translations, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_list = []\n",
    "for i, filename in enumerate(os.listdir(SPLIT_TEXT_FILES_DIR)):\n",
    "    with open(os.path.join(SPLIT_TEXT_FILES_DIR, filename), \"r\") as fp:\n",
    "        text = fp.read()\n",
    "\n",
    "    paragraphs = list(text.split(paragraph_delimiter))\n",
    "    for j, paragraph in enumerate(paragraphs):\n",
    "        par_list.append({\n",
    "            \"id\": int(filename.split(\".\")[0]),\n",
    "            \"paragraph\": j,\n",
    "            \"text\": paragraph\n",
    "        })\n",
    "df_paragraphs = pd.DataFrame(par_list).pivot(index=\"id\", columns=\"paragraph\", values=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate googletrans translations and langdetect detections\n",
    "\n",
    "Produce dataframes with documents in rows and paragraphs in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_google = df_translations.pivot(index=\"id\", columns=\"paragraph\", values=\"gt_trans\")\n",
    "df_det_langdet = df_translations.pivot(index=\"id\", columns=\"paragraph\", values=\"ld_lang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get googletrans translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See where googletrans failed\n",
    "failed_translation_map = df_tr_google.applymap(lambda x: x is None)\n",
    "\n",
    "df_translations = df_tr_google.applymap(lambda t: t.text if not pd.isna(t) else None)\n",
    "\n",
    "# Where translations failed, fill with originals\n",
    "df_trans_filled = df_translations.fillna(df_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What languages were the paragraphs originally in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_src_langs = df_tr_google.applymap(lambda x: x.src if not pd.isna(x) else None)\n",
    "\n",
    "ld_src_langs = df_det_langdet.applymap(lambda x: x[0].lang if isinstance(x, list) else None)\n",
    "df_ld_lens = df_det_langdet.applymap(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_langs_per_doc = gt_src_langs.apply(lambda r: set(r) - set([\"en\", None]), axis=1)\n",
    "ld_langs_per_doc = ld_src_langs.apply(lambda r: set(r) - set([\"en\", None]), axis=1)\n",
    "\n",
    "# What non-English languages were there?\n",
    "# gt_langs_per_doc[~(gt_langs_per_doc == set([\"en\"]))]\n",
    "# set(ld_langs_per_doc[~(ld_langs_per_doc == set([\"en\"]))].apply(tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What languages were the paragraphs we could not translate in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    160\n",
       "fr     12\n",
       "es      4\n",
       "ar      1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "missed_paragraph_langs = pd.Series(np.ravel(ld_src_langs.where(failed_translation_map).values)).value_counts()\n",
    "display(missed_paragraph_langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-English languages and texts in those languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Not necessarily all) texts with (possibly) more than one language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATIONS_TEXT_DIR = os.path.join(TRANSLATIONS_DIR, \"texts\")\n",
    "if os.path.exists(TRANSLATIONS_TEXT_DIR):\n",
    "    shutil.rmtree(TRANSLATIONS_TEXT_DIR)\n",
    "os.makedirs(TRANSLATIONS_TEXT_DIR)\n",
    "\n",
    "for doc_id, row in df_trans_filled.iterrows():\n",
    "    out_text = paragraph_delimiter.join(row.dropna())\n",
    "    \n",
    "    with open(os.path.join(TRANSLATIONS_TEXT_DIR, f\"{doc_id}.txt\"), \"w\") as fp:\n",
    "        fp.write(out_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches.to_csv(os.path.join(TRANSLATIONS_DIR, \"document_data.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
