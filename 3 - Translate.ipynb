{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from time import sleep, time\n",
    "\n",
    "import requests\n",
    "\n",
    "# Data sheets \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Language\n",
    "from googletrans import Translator\n",
    "# Language detection\n",
    "import langdetect\n",
    "langdetect.DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \".\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "\n",
    "# Where we will store data from all sources after combining it\n",
    "ALL_DATA_DIR = os.path.join(DATA_DIR, \"combined\")\n",
    "# This will hold the converted versions of the PDF documents\n",
    "SPLIT_TEXT_FILES_DIR = os.path.join(ALL_DATA_DIR, \"text_files_split\")\n",
    "# This will hold everything related to translating text, including the results of the translations\n",
    "TRANSLATIONS_DIR = os.path.join(ALL_DATA_DIR, \"translations\")\n",
    "\n",
    "if not os.path.exists(TRANSLATIONS_DIR):\n",
    "    os.makedirs(TRANSLATIONS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_delimiter = \"\\n\\n\" + \"=\" * 20 + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Type</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>link</th>\n",
       "      <th>scanned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accountability for Perpetrators: UN Officials ...</td>\n",
       "      <td>Official Statement</td>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>UN Special Representative of the Secretary-Gen...</td>\n",
       "      <td>https://www.globalr2p.org/wp-content/uploads/2...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title                Type  \\\n",
       "id                                                                          \n",
       "0   Accountability for Perpetrators: UN Officials ...  Official Statement   \n",
       "\n",
       "         Date                                             Source  \\\n",
       "id                                                                 \n",
       "0  2019-11-07  UN Special Representative of the Secretary-Gen...   \n",
       "\n",
       "                                                 link  scanned  \n",
       "id                                                              \n",
       "0   https://www.globalr2p.org/wp-content/uploads/2...    False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speeches = pd.read_csv(os.path.join(ALL_DATA_DIR, \"document_data.csv\"), index_col=\"id\")\n",
    "df_speeches.Date = pd.to_datetime(df_speeches[\"Date\"])\n",
    "df_speeches.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate /detect languages in paragraphs\n",
    "\n",
    "Not all texts are in English (or not all of them. We can try and automatically detect which ones contain text in another language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing_translations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 50 out of 947 in 23.4 seconds\n",
      "Done with 100 out of 947 in 38.0 seconds\n",
      "\tFailed to translate for 540.txt, paragraph 7. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tNo internet connection!\n",
      "\tNo internet connection!\n",
      "Done with 150 out of 947 in 64.9 seconds\n",
      "Done with 200 out of 947 in 93.0 seconds\n",
      "\tFailed to translate for 320.txt, paragraph 11. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 320.txt, paragraph 11. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 320.txt, paragraph 11\n",
      "Done with 250 out of 947 in 129.1 seconds\n",
      "Done with 300 out of 947 in 149.2 seconds\n",
      "\tlangdetect could not detect languages in 526.txt, paragraph 22\n",
      "Done with 350 out of 947 in 159.7 seconds\n",
      "\tlangdetect could not detect languages in 532.txt, paragraph 1\n",
      "\tFailed to translate for 532.txt, paragraph 1. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 532.txt, paragraph 1. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 532.txt, paragraph 1\n",
      "Done with 400 out of 947 in 188.2 seconds\n",
      "Done with 450 out of 947 in 217.3 seconds\n",
      "Done with 500 out of 947 in 225.9 seconds\n",
      "\tlangdetect could not detect languages in 77.txt, paragraph 11\n",
      "Done with 550 out of 947 in 234.3 seconds\n",
      "\tlangdetect could not detect languages in 181.txt, paragraph 21\n",
      "Done with 600 out of 947 in 252.5 seconds\n",
      "\tFailed to translate for 543.txt, paragraph 12. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 12. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 12\n",
      "\tFailed to translate for 543.txt, paragraph 13. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 13. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 13\n",
      "\tFailed to translate for 543.txt, paragraph 14. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 14. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 14\n",
      "\tFailed to translate for 543.txt, paragraph 15. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 15. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 15\n",
      "\tFailed to translate for 543.txt, paragraph 16. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 16. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 16\n",
      "\tFailed to translate for 543.txt, paragraph 17. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 17. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 17\n",
      "\tFailed to translate for 543.txt, paragraph 18. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 18. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 18\n",
      "\tFailed to translate for 543.txt, paragraph 19. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 19. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 19\n",
      "\tFailed to translate for 543.txt, paragraph 20. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 20. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 20\n",
      "\tFailed to translate for 543.txt, paragraph 21. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 21. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 21\n",
      "\tFailed to translate for 543.txt, paragraph 22. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 22. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 22\n",
      "\tFailed to translate for 543.txt, paragraph 23. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 23. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 23\n",
      "\tFailed to translate for 543.txt, paragraph 24. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 24. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 24\n",
      "\tFailed to translate for 543.txt, paragraph 25. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 25. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 25\n",
      "\tFailed to translate for 543.txt, paragraph 26. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 26. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 26\n",
      "\tFailed to translate for 543.txt, paragraph 27. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 27. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 27\n",
      "\tFailed to translate for 543.txt, paragraph 28. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 28. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 28\n",
      "\tFailed to translate for 543.txt, paragraph 29. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 29. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 29\n",
      "\tFailed to translate for 543.txt, paragraph 30. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 30. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 30\n",
      "\tFailed to translate for 543.txt, paragraph 31. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 31. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 31\n",
      "\tFailed to translate for 543.txt, paragraph 32. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 543.txt, paragraph 32. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 543.txt, paragraph 32\n",
      "\tFailed to translate for 575.txt, paragraph 1. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 575.txt, paragraph 1. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 575.txt, paragraph 1\n",
      "\tFailed to translate for 575.txt, paragraph 2. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 575.txt, paragraph 2. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 575.txt, paragraph 2\n",
      "\tFailed to translate for 575.txt, paragraph 3. Will re-connect and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 575.txt, paragraph 3. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 575.txt, paragraph 3\n",
      "\tFailed to translate for 575.txt, paragraph 4. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 575.txt, paragraph 4. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 575.txt, paragraph 4\n",
      "\tFailed to translate for 379.txt, paragraph 1. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 379.txt, paragraph 1. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 379.txt, paragraph 1\n",
      "\tFailed to translate for 379.txt, paragraph 2. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 379.txt, paragraph 2. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 379.txt, paragraph 2\n",
      "\tFailed to translate for 379.txt, paragraph 3. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 379.txt, paragraph 3. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 379.txt, paragraph 3\n",
      "\tFailed to translate for 379.txt, paragraph 4. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 379.txt, paragraph 4. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 379.txt, paragraph 4\n",
      "\tFailed to translate for 379.txt, paragraph 5. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 379.txt, paragraph 5. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "\tgoogletrans could not detect languages in 379.txt, paragraph 5\n",
      "\tFailed to translate for 379.txt, paragraph 6. Will re-connect and retry\n",
      "\tConnected successfully, attempt 1 | 3\n",
      "\tFailed to translate for 379.txt, paragraph 6. Will re-connect and retry\n",
      "\tConnected successfully, attempt 2 | 3\n",
      "Done with 650 out of 947 in 578.4 seconds\n",
      "Done with 700 out of 947 in 613.4 seconds\n",
      "Done with 750 out of 947 in 633.0 seconds\n",
      "Done with 800 out of 947 in 681.2 seconds\n",
      "Done with 850 out of 947 in 707.0 seconds\n",
      "Done with 900 out of 947 in 717.1 seconds\n",
      "\tNo internet connection!\n",
      "CPU times: user 1min 29s, sys: 2.71 s, total: 1min 31s\n",
      "Wall time: 12min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Translation takes a while, so if we have don it before, load\n",
    "if load_existing_translations:\n",
    "    with open(os.path.join(TRANSLATIONS_DIR, \"lang_translations_df.pkl\"), \"rb\") as fp:\n",
    "        df_translations = pickle.load(fp)\n",
    "        translations = df_translations.to_dict(\"records\")\n",
    "\n",
    "else:\n",
    "    # Instantiate googletrans translator object\n",
    "    translator = Translator(timeout=5)\n",
    "    from json import JSONDecodeError\n",
    "    from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "\n",
    "    translations = []\n",
    "    nfiles = len(os.listdir(SPLIT_TEXT_FILES_DIR))\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(SPLIT_TEXT_FILES_DIR)):\n",
    "        with open(os.path.join(SPLIT_TEXT_FILES_DIR, filename), \"r\") as fp:\n",
    "            text = fp.read()\n",
    "\n",
    "        paragraphs = list(text.split(paragraph_delimiter))\n",
    "\n",
    "        for j, paragraph in enumerate(paragraphs):\n",
    "\n",
    "            gt_trans = None\n",
    "            ld_lang = None\n",
    "            \n",
    "            # Langdetect\n",
    "            try:\n",
    "                ld_lang = langdetect.detect_langs(paragraph)\n",
    "            except LangDetectException:\n",
    "                print(f\"\\tlangdetect could not detect languages in {filename}, paragraph {j+1}\")\n",
    "            \n",
    "            # Only translate texts for which we are not sure they are in English\n",
    "            must_translate = True\n",
    "            if ld_lang is not None and len(ld_lang) == 1 and ld_lang[0].lang == \"en\":\n",
    "                 must_translate = False\n",
    "            \n",
    "            if must_translate:\n",
    "                # googletrans: need retry logic since it fails sometimes\n",
    "                n_tries = 3\n",
    "                for i_try in range(n_tries):\n",
    "\n",
    "                    connected = False\n",
    "                    # Make sure we have internet. Do not continue unless we do\n",
    "                    while not connected:\n",
    "                        try:\n",
    "                            _ = requests.get(\"https://www.google.com\", timeout=1)\n",
    "                            connected = True\n",
    "                        except:\n",
    "                            print(\"\\tNo internet connection!\")\n",
    "                            sleep(1)\n",
    "\n",
    "                    # Attempt to translate\n",
    "                    try:\n",
    "                        gt_trans = translator.translate(paragraph)\n",
    "                        break\n",
    "                    except JSONDecodeError:\n",
    "                        if i_try < n_tries - 1:\n",
    "                            err_text = \"Will re-connect and retry\"\n",
    "                            print(f\"\\tFailed to translate for {filename}, paragraph {j+1}. {err_text}\")\n",
    "\n",
    "                            # Google has blocked the current IP address, switch that\n",
    "                            subprocess.run([\"nordvpn\", \"d\"], check=True)\n",
    "                            subprocess.run([\"nordvpn\", \"c\", \"be\"], check=True)\n",
    "                            print(f\"\\tConnected successfully, attempt {i_try+1} | {n_tries}\")\n",
    "                            # Sometimes it takes a while for a connection to be actually established, hang for a bit\n",
    "                            sleep(1)\n",
    "\n",
    "                            # Important: need to re-instantiate the translator object after changing IP\n",
    "                            translator = Translator(timeout=5)\n",
    "                        else:\n",
    "                            print(f\"\\tgoogletrans could not detect languages in {filename}, paragraph {j+1}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\tSomething went wrong {e}. Will retry.\")\n",
    "\n",
    "\n",
    "            row = {\n",
    "                \"id\": int(filename.split(\".\")[0]),\n",
    "                \"paragraph\": j,\n",
    "                \"gt_trans\": gt_trans,\n",
    "                \"ld_lang\": ld_lang,\n",
    "                \"translated\": must_translate,\n",
    "            }\n",
    "\n",
    "            translations.append(row)\n",
    "\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"Done with {i+1} out of {nfiles} in {time() - start:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translations = pd.DataFrame(translations)\n",
    "\n",
    "with open(os.path.join(TRANSLATIONS_DIR, \"lang_translations_df.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(df_translations, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_list = []\n",
    "for i, filename in enumerate(os.listdir(SPLIT_TEXT_FILES_DIR)):\n",
    "    with open(os.path.join(SPLIT_TEXT_FILES_DIR, filename), \"r\") as fp:\n",
    "        text = fp.read()\n",
    "\n",
    "    paragraphs = list(text.split(paragraph_delimiter))\n",
    "    for j, paragraph in enumerate(paragraphs):\n",
    "        par_list.append({\n",
    "            \"id\": int(filename.split(\".\")[0]),\n",
    "            \"paragraph\": j,\n",
    "            \"text\": paragraph\n",
    "        })\n",
    "        \n",
    "df_paragraphs_raw = pd.DataFrame(par_list)\n",
    "df_paragraphs = df_paragraphs_raw.merge(df_translations, on=[\"id\", \"paragraph\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix missing translations\n",
    "\n",
    "We sometimes fail for some paragraphs, perhaps because they are too long. Re-do the translation, splitting long paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See where googletrans failed\n",
    "df_paragraphs[\"failed\"] = df_paragraphs[\"gt_trans\"].isna() & df_paragraphs[\"translated\"]\n",
    "\n",
    "df_translations_fixed = df_translations.copy(deep=True)\n",
    "\n",
    "translator = Translator(timeout=5)\n",
    "\n",
    "for row_id, row in df_paragraphs[df_paragraphs[\"failed\"]].iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    text_ix = 0\n",
    "    \n",
    "    translated_text = \"\"\n",
    "    \n",
    "    while text_ix < len(text):\n",
    "        chunk = text[text_ix:text_ix + 2000]\n",
    "        \n",
    "        gt_trans = translator.translate(paragraph)\n",
    "        \n",
    "        translated_text += gt_trans.text\n",
    "        \n",
    "        text_ix += 2000\n",
    "    \n",
    "    gt_trans.text = translated_text\n",
    "    df_translations_fixed.loc[row_id, \"gt_trans\"] = gt_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save translations (with additions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(TRANSLATIONS_DIR, \"lang_translations_df.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(df_translations, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new paragraphs dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs = df_paragraphs_raw.merge(df_translations_fixed, on=[\"id\", \"paragraph\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What languages were the paragraphs originally in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs[\"gt_det_lang\"] = df_paragraphs[\"gt_trans\"].apply(lambda x: x.src if not pd.isna(x) else None)\n",
    "df_paragraphs[\"ld_det_lang\"] = df_paragraphs[\"ld_lang\"].apply(lambda x: x[0].lang if isinstance(x, list) else None)\n",
    "\n",
    "# How many different languages might have been detected?\n",
    "df_paragraphs[\"ld_n_langs\"] = df_paragraphs[\"ld_lang\"].apply(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get googletrans translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs[\"translation\"] = df_paragraphs[\"gt_trans\"].apply(lambda t: t.text if not pd.isna(t) else None)\n",
    "\n",
    "# Where translations failed, fill with originals\n",
    "df_paragraphs[\"translation\"] = df_paragraphs[\"translation\"].fillna(df_paragraphs[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_langs = df_paragraphs[df_paragraphs[\"gt_det_lang\"] != df_paragraphs[\"ld_det_lang\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-English languages and texts in those languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([474, 741, 651, 386, 318, 496, 331, 753, 740, 328, 578, 540, 242,\n",
       "       340, 714, 420, 534, 206, 653, 361, 611, 493, 410, 320, 322, 291,\n",
       "       396, 308,   0, 468, 475, 701, 381, 294, 526,  96, 323, 532, 345,\n",
       "       511, 564, 635, 721, 529, 579, 622, 329, 678, 432, 479, 566, 571,\n",
       "       470, 706, 576, 535,  77, 417,  10, 558, 598, 645, 391, 181, 719,\n",
       "       590, 543, 575, 379, 111, 626, 503, 567, 607, 312, 694, 518, 380,\n",
       "       275, 448, 554, 281, 742, 650, 338, 360, 697, 602, 733, 280, 354,\n",
       "       488,   7, 597, 755, 720, 541, 675, 462, 445, 621])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_english = df_paragraphs[\n",
    "    (df_paragraphs[\"ld_det_lang\"] != \"en\") |\n",
    "    ((df_paragraphs[\"gt_det_lang\"] != \"en\") & (~df_paragraphs[\"gt_det_lang\"].isna()))\n",
    "]\n",
    "\n",
    "non_english[\"id\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Not necessarily all) texts with (possibly) more than one language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs[\"src_lang\"] = df_paragraphs[\"gt_det_lang\"].fillna(df_paragraphs[\"ld_det_lang\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_langs_per_doc = df_paragraphs.groupby(\"id\")[\"src_lang\"].nunique()\n",
    "multilingual_docs = n_langs_per_doc[n_langs_per_doc > 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  0,   7,  10, 291, 318, 360, 361, 379, 432, 470, 474, 479, 503,\n",
       "            518, 534, 540, 541, 543, 590, 597, 607, 626, 645, 678, 697, 719,\n",
       "            740, 741],\n",
       "           dtype='int64', name='id')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches[\"multilingual\"] = False\n",
    "df_speeches.at[multilingual_docs, \"multilingual\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove (un)translated paragraphs in bilingual documents\n",
    "\n",
    "Some documents include an official translation. For those, only keep the paragraphs with the translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of paragraphs for each language for bilingual documents\n",
    "\n",
    "df_lan_counts = pd.DataFrame(df_paragraphs.groupby(\"id\")[\"src_lang\"].value_counts())\n",
    "df_lan_counts.columns = [\"count\"]\n",
    "df_lan_counts.reset_index(inplace=True)\n",
    "df_lan_counts = df_lan_counts[df_lan_counts[\"id\"].isin(multilingual_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to signify whether this paragraph should be discarded or not\n",
    "df_paragraphs[\"to_keep\"] = True\n",
    "\n",
    "for doc_id in df_lan_counts.id.unique():\n",
    "    subset = df_lan_counts[df_lan_counts[\"id\"] == doc_id]\n",
    "    \n",
    "    counts = subset[[\"src_lang\", \"count\"]].set_index(\"src_lang\").to_dict(\"dict\")[\"count\"]\n",
    "    \n",
    "    if not \"en\" in counts:\n",
    "        continue\n",
    "    \n",
    "    n_paragraphs = sum(counts.values())\n",
    "    \n",
    "    keep_english = False\n",
    "    if counts[\"en\"] / n_paragraphs > 0.33:\n",
    "        keep_english = True\n",
    "    \n",
    "    if keep_english:\n",
    "        df_paragraphs.at[\n",
    "            (df_paragraphs[\"id\"] == doc_id) & ~(df_paragraphs[\"src_lang\"] == \"en\"), \"to_keep\"] = False\n",
    "    else:\n",
    "        df_paragraphs.at[\n",
    "            (df_paragraphs[\"id\"] == doc_id) & (df_paragraphs[\"src_lang\"] == \"en\"), \"to_keep\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATIONS_TEXT_DIR = os.path.join(TRANSLATIONS_DIR, \"texts\")\n",
    "if os.path.exists(TRANSLATIONS_TEXT_DIR):\n",
    "    shutil.rmtree(TRANSLATIONS_TEXT_DIR)\n",
    "os.makedirs(TRANSLATIONS_TEXT_DIR)\n",
    "\n",
    "\n",
    "# Make a DOCxPAR matrix \n",
    "df_trans_filled = df_paragraphs[df_paragraphs[\"to_keep\"]].pivot(\n",
    "    index=\"id\", columns=\"paragraph\", values=\"translation\")\n",
    "\n",
    "for doc_id, row in df_trans_filled.iterrows():\n",
    "    out_text = paragraph_delimiter.join(row.dropna())\n",
    "    \n",
    "    with open(os.path.join(TRANSLATIONS_TEXT_DIR, f\"{doc_id}.txt\"), \"w\") as fp:\n",
    "        fp.write(out_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches.to_csv(os.path.join(TRANSLATIONS_DIR, \"document_data.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
